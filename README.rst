Scratchpad for Pocketsphinx-based Linux Voice Dictation
=======================================================

What's the Idea?
================

The basic idea is to see if CMU Pocketsphinx (a BSD-licensed continuous 
voice recognition system) can be used to create a programmer's file editor
that uses voice dictation not to eliminate use of the hands, but to allow them
to be used far less by allowing text entry. While editing/navigation/command 
and control will likely be looked at some time, they aren't a priority.

Big Caveats:

 * This is a *very* early stage project
 * Seriously, it *does not* do anything yet
 * This project is currently English-only (and English-US keyboard only). 
   It would be nice to support other languages, but it is not a priority for me
 * This project is Linux-only.I am targetting modern (K)ubuntu desktops.
 * The use of pocketsphinx is actually somewhat incidental. While we are using 
   pocketsphinx right now, we should be able to switch out the engine for 
   another at some point with little change to the GUI and related services.
   The reason I'm using Pocketsphinx is that it comes nicely packaged under 
   Ubuntu and provides pre-packaged language models for English-US dictation.

Setup
=====

Dependencies::

    $ apt-get install gstreamer0.10-pocketsphinx build-essential \
        libsphinxbase1 sphinxbase-utils sphinxtrain \
        pocketsphinx-hmm-en-hub4wsj pocketsphinx-utils \
        espeak
    
    # for the Qt-based GUI
    $ apt-get install python-pyside.qtcore python-pyside.qtwebkit python-jinja2

    # for the Desktop service (uinput), currently unimplemented
    $ apt-get install python-dbus

Listener is a python library (using setuptools), use::

    setup.py develop --user

to install.

Utilities
=========

`listener-pipe`

    Attempt to setup a gstreamer pipeline using a downloaded language model 
    that matches the hub4wsj package. 
    The pipeline will store utterances into 
    `~/.config/listener/default/recordings` 
    and print out the partial and final results to the console.

`listener-rawplay <filename>`

    Plays a raw audio file as output by the listener-pipe into the 
    recording directory (to allow the user to review the content before 
    adding it to their training data-set)

`echo "word" | listener-arpa`

    Prints out the best-guess ARPABet definition for the incoming word,
    these are the things you need to add to a '.dict' file for pocketsphinx,
    generated by extracting correspondences between espeak and the CMU 
    dictionary project data-files.

`listener-uinput-device`

    Test case that tries to do a uinput "send keys like" operation,
    operates at the Linux kernel uinput level, so should work with 
    any environment (in theory it could even work on a console, though 
    I have not tried that).

Internal Utilities 
------------------

These just modify (json) structures that are part of the code-base that 
provide lookup tables used by the code.
    
`listener-uinput-rebuild-mapping`

    Rebuilds the mapping from character to keystrokes. Currently this 
    just reads a kernel header and applies some hand-coded keyboard 
    mappings for a US-english keyboard. Eventually should use users 
    local xkb mappings (including compose keys) to properly map characters.

`listener-ipa-arpa-statmap`

    Re-extract IPA -> ARPABet statistical map, should the algorithm 
    be improved

Where are we Going?
===================
    
The model is (and this is just a sketch so far):

    * use Pocketsphinx running under gstreamer 
    
        * we could use pocketsphinx directly, but gstreamer gives us nice 
          features for pre and post-processing the audio if necessary
          however, so far those features don't actually work, at least they 
          don't seem to work wrt storing data etc.

    * record each utterance into ram-disk (or disk)
    
        * currently this is being done in the vader component
          and is just being dumped to home directory rather than ram disk
    
    * provide "correct that" style training/grammar updates
    
        * use the already-uttered sound-file to do the training
        * train acousticly *and* update language model 
        * acoustic training relies on have a reliable transcription
        * transcription requires that each "word" be in the dictionary with 
          phonetic translation
          
            * when new words are encountered we will need to help the user 
              convert them to phonetics (preferably without their needing to 
              work with the phonetic alphabet in the normal case, but allowing 
              them to see what is being used and override/fix it when necessary)
            * code is written to allow for use of espeak phonetic output to 
              produce loose ARPABet translations
        
        * plan to allow for "upload your utterances" functionality, so that 
          a user can upload non-private utterances as voice-training data 
          (with the transcription).
    
    * on opening a project (git/bzr/hg repository)
        * scan the project source code and convert to dictation words
        * build a language model from that translation
        * layer the project-specific language model onto a generic natural-language model
    
    * similarly, allow for e.g. "read my mail" functionality so that we can parse a 
      user's (sent) email to get an idea of how they normally speak
    
    * apply interpretation at a higher level
    
        * if there are 10 possible matches, given context, which one would make the most sense?
        * apply "sounds like" filtering to get more possible matches? (hopefully not required)
        
    * ideally, be able to switch between fine-grained models such that saying "from " would 
      trigger a switch to a new context such that a different sphinx would then process the 
      module name. This is really a fluid set, we want layers of models and the ability to 
      swap them out as context changes (e.g. when you navigate into a method, you want the 
      variables in that method to become very likely dictation targets, with class methods,
      module identifiers etc coming in behind)
      
        * "identifiers" 
        * classes
        * modules
    
    * possibly figure out how to include the "context" in the model when processing hmms,
      such that sphinx could see context as a known-state value in the HMM?
    
    * Recording level is *very* important for pocketsphinx; 
      too loud and you'll have an infinitely long 
      utterance where every bit of background is considered speech; too soft 
      and you'll just get random junk where only the loudest bits of speech 
      are processed.
      
        * Need to provide volume control as part of the setup/checking,
          possibly even include a "say nothing for a moment, now say this" setup 
          so that we can dynamically adjust to messy environments
        
